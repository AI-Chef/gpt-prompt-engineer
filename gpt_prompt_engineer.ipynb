{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMliR+tlICmgpTIGacdbI4d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshumer/gpt-prompt-engineer/blob/main/gpt_prompt_engineer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gpt-prompt-engineer\n",
        "By Matt Shumer (https://twitter.com/mattshumer_)\n",
        "\n",
        "Github repo: https://github.com/mshumer/gpt-prompt-engineer\n",
        "\n",
        "Generate an optimal prompt for a given task.\n",
        "\n",
        "To generate a prompt:\n",
        "1. In the first cell, add in your OpenAI key.\n",
        "2. In the last cell, fill in the description of your task, up to 15 test cases, and the number of prompts to generate.\n",
        "3. Run all the cells! The AI will generate a number of candidate prompts, and test them all to find the best one!"
      ],
      "metadata": {
        "id": "WljjH8K3s7kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install prettytable\n",
        "!pip install tqdm\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "import time\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "\n",
        "openai.api_key = \"ADD YOUR KEY HERE\" # enter your OpenAI API key here"
      ],
      "metadata": {
        "id": "dQmMZdkG_RA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K is a constant factor that determines how much ratings change\n",
        "K = 32\n",
        "\n",
        "def generate_candidate_prompts(description, test_cases, number_of_prompts):\n",
        "  outputs = openai.ChatCompletion.create(\n",
        "      model='gpt-4', # change this to gpt-3.5-turbo if you don't have GPT-4 access\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"\"\"Your job is to generate system prompts for GPT-4, given a description of the use-case and some test cases.\n",
        "\n",
        "The prompts you will be generating will be for freeform tasks, such as generating a landing page headline, an intro paragraph, solving a math problem, etc.\n",
        "\n",
        "In your generated prompt, you should describe how the AI should behave in plain English. Include what it will see, and what it's allowed to output. Be creative with prompts to get the best possible results. The AI knows it's an AI -- you don't need to tell it this.\n",
        "\n",
        "You will be graded based on the performance of your prompt... but don't cheat! You cannot include specifics about the test cases in your prompt. Any prompts with examples will be disqualified.\n",
        "\n",
        "Most importantly, output NOTHING but the prompt. Do not include anything else in your message.\"\"\"},\n",
        "          {\"role\": \"user\", \"content\": f\"Here are some test cases:`{test_cases}`\\n\\nHere is the description of the use-case: `{description.strip()}`\\n\\nRespond with your prompt, and nothing else. Be creative.\"}\n",
        "          ],\n",
        "      temperature=.9,\n",
        "      n=number_of_prompts)\n",
        "\n",
        "  prompts = []\n",
        "\n",
        "  for i in outputs.choices:\n",
        "    prompts.append(i.message.content)\n",
        "  return prompts\n",
        "\n",
        "def expected_score(r1, r2):\n",
        "    return 1 / (1 + 10**((r2 - r1) / 400))\n",
        "\n",
        "def update_elo(r1, r2, score1):\n",
        "    e1 = expected_score(r1, r2)\n",
        "    e2 = expected_score(r2, r1)\n",
        "    return r1 + K * (score1 - e1), r2 + K * ((1 - score1) - e2)\n",
        "\n",
        "def test_candidate_prompts(test_cases, description, prompts):\n",
        "  # Initialize each prompt with an ELO rating of 1200\n",
        "  prompt_ratings = {prompt: 1200 for prompt in prompts}\n",
        "\n",
        "  # Calculate total rounds for progress bar\n",
        "  total_rounds = len(test_cases) * len(prompts) * (len(prompts) - 1) // 2\n",
        "\n",
        "  # Initialize progress bar\n",
        "  pbar = tqdm(total=total_rounds, ncols=70)\n",
        "\n",
        "  # For each pair of prompts\n",
        "  for prompt1, prompt2 in itertools.combinations(prompts, 2):\n",
        "      # For each test case\n",
        "      for test_case in test_cases:\n",
        "          # Update progress bar\n",
        "          pbar.update()\n",
        "\n",
        "          # Generate outputs for each prompt\n",
        "          generation1 = openai.ChatCompletion.create(\n",
        "              model='gpt-3.5-turbo',\n",
        "              messages=[\n",
        "                  {\"role\": \"system\", \"content\": prompt1},\n",
        "                  {\"role\": \"user\", \"content\": f\"{test_case['prompt']}\"}\n",
        "              ],\n",
        "              max_tokens=60,\n",
        "              temperature=0.8,\n",
        "          ).choices[0].message.content\n",
        "\n",
        "          generation2 = openai.ChatCompletion.create(\n",
        "              model='gpt-3.5-turbo',\n",
        "              messages=[\n",
        "                  {\"role\": \"system\", \"content\": prompt2},\n",
        "                  {\"role\": \"user\", \"content\": f\"{test_case['prompt']}\"}\n",
        "              ],\n",
        "              max_tokens=60,\n",
        "              temperature=0.8,\n",
        "          ).choices[0].message.content\n",
        "\n",
        "          # Rank the outputs\n",
        "          try:\n",
        "            score1 = openai.ChatCompletion.create(\n",
        "                model='gpt-3.5-turbo',\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"\"\"Your job is to rank the quality of two outputs generated by different prompts. The prompts are used to generate a response for a given task.\n",
        "\n",
        "You will be provided with the task description, the test prompt, and two generations - one for each system prompt.\n",
        "\n",
        "Rank the generations in order of quality. If Generation A is better, respond with 'A'. If Generation B is better, respond with 'B'.\n",
        "\n",
        "Remember, to be considered 'better', a generation must not just be good, it must be noticeably superior to the other.\n",
        "\n",
        "Also, keep in mind that you are a very harsh critic. Only rank a generation as better if it truly impresses you more than the other.\n",
        "\n",
        "Respond with your ranking, and nothing else. Be fair and unbiased in your judgement.\"\"\"},\n",
        "                  {\"role\": \"user\", \"content\": f\"\"\"Task: {description.strip()}\n",
        "Prompt: {test_case['prompt']}\n",
        "Generation A: {generation1}\n",
        "Generation B: {generation2}\"\"\"}\n",
        "                ],\n",
        "                logit_bias={\n",
        "                      '32': 100,  # 'A' token\n",
        "                      '33': 100,  # 'B' token\n",
        "                  },\n",
        "                max_tokens=1,\n",
        "                temperature=0,\n",
        "            ).choices[0].message.content\n",
        "          except:\n",
        "            time.sleep(61)\n",
        "            score1 = openai.ChatCompletion.create(\n",
        "                model='gpt-3.5-turbo',\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"\"\"Your job is to rank the quality of two outputs generated by different prompts. The prompts are used to generate a response for a given task.\n",
        "\n",
        "You will be provided with the task description, the test prompt, and two generations - one for each system prompt.\n",
        "\n",
        "Rank the generations in order of quality. If Generation A is better, respond with 'A'. If Generation B is better, respond with 'B'.\n",
        "\n",
        "Remember, to be considered 'better', a generation must not just be good, it must be noticeably superior to the other.\n",
        "\n",
        "Also, keep in mind that you are a very harsh critic. Only rank a generation as better if it truly impresses you more than the other.\n",
        "\n",
        "Respond with your ranking, and nothing else. Be fair and unbiased in your judgement.\"\"\"},\n",
        "                  {\"role\": \"user\", \"content\": f\"\"\"Task: {description.strip()}\n",
        "Prompt: {test_case['prompt']}\n",
        "Generation A: {generation1}\n",
        "Generation B: {generation2}\"\"\"}\n",
        "                ],\n",
        "                logit_bias={\n",
        "                      '32': 100,  # 'A' token\n",
        "                      '33': 100,  # 'B' token\n",
        "                  },\n",
        "                max_tokens=1,\n",
        "                temperature=0,\n",
        "            ).choices[0].message.content\n",
        "\n",
        "\n",
        "          try:\n",
        "            score2 = openai.ChatCompletion.create(\n",
        "                model='gpt-3.5-turbo',\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"\"\"Your job is to rank the quality of two outputs generated by different prompts. The prompts are used to generate a response for a given task.\n",
        "\n",
        "You will be provided with the task description, the test prompt, and two generations - one for each system prompt.\n",
        "\n",
        "Rank the generations in order of quality. If Generation A is better, respond with 'A'. If Generation B is better, respond with 'B'.\n",
        "\n",
        "Remember, to be considered 'better', a generation must not just be good, it must be noticeably superior to the other.\n",
        "\n",
        "Also, keep in mind that you are a very harsh critic. Only rank a generation as better if it truly impresses you more than the other.\n",
        "\n",
        "Respond with your ranking, and nothing else. Be fair and unbiased in your judgement.\"\"\"},\n",
        "                  {\"role\": \"user\", \"content\": f\"\"\"Task: {description.strip()}\n",
        "Prompt: {test_case['prompt']}\n",
        "Generation A: {generation2}\n",
        "Generation B: {generation1}\"\"\"}\n",
        "                ],\n",
        "                logit_bias={\n",
        "                      '32': 100,  # 'A' token\n",
        "                      '33': 100,  # 'B' token\n",
        "                  },\n",
        "                max_tokens=1,\n",
        "                temperature=.5,\n",
        "            ).choices[0].message.content\n",
        "          except:\n",
        "            time.sleep(61)\n",
        "            score2 = openai.ChatCompletion.create(\n",
        "                model='gpt-3.5-turbo',\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"\"\"Your job is to rank the quality of two outputs generated by different prompts. The prompts are used to generate a response for a given task.\n",
        "\n",
        "You will be provided with the task description, the test prompt, and two generations - one for each system prompt.\n",
        "\n",
        "Rank the generations in order of quality. If Generation A is better, respond with 'A'. If Generation B is better, respond with 'B'.\n",
        "\n",
        "Remember, to be considered 'better', a generation must not just be good, it must be noticeably superior to the other.\n",
        "\n",
        "Also, keep in mind that you are a very harsh critic. Only rank a generation as better if it truly impresses you more than the other.\n",
        "\n",
        "Respond with your ranking, and nothing else. Be fair and unbiased in your judgement.\"\"\"},\n",
        "                  {\"role\": \"user\", \"content\": f\"\"\"Task: {description.strip()}\n",
        "Prompt: {test_case['prompt']}\n",
        "Generation A: {generation2}\n",
        "Generation B: {generation1}\"\"\"}\n",
        "                ],\n",
        "                logit_bias={\n",
        "                      '32': 100,  # 'A' token\n",
        "                      '33': 100,  # 'B' token\n",
        "                  },\n",
        "                max_tokens=1,\n",
        "                temperature=.5,\n",
        "            ).choices[0].message.content\n",
        "\n",
        "          # Convert scores to numeric values\n",
        "          score1 = 1 if score1 == 'A' else 0 if score1 == 'B' else 0.5\n",
        "          score2 = 1 if score2 == 'B' else 0 if score2 == 'A' else 0.5\n",
        "\n",
        "          # Average the scores\n",
        "          score = (score1 + score2) / 2\n",
        "\n",
        "          # Update ELO ratings\n",
        "          r1, r2 = prompt_ratings[prompt1], prompt_ratings[prompt2]\n",
        "          r1, r2 = update_elo(r1, r2, score)\n",
        "          prompt_ratings[prompt1], prompt_ratings[prompt2] = r1, r2\n",
        "\n",
        "          # Print the winner of this round\n",
        "          if score > 0.5:\n",
        "              print(f\"Winner: {prompt1}\")\n",
        "          elif score < 0.5:\n",
        "              print(f\"Winner: {prompt2}\")\n",
        "          else:\n",
        "              print(\"Draw\")\n",
        "\n",
        "  # Close progress bar\n",
        "  pbar.close()\n",
        "\n",
        "  return prompt_ratings\n",
        "\n",
        "\n",
        "\n",
        "def generate_optimal_prompt(description, test_cases, number_of_prompts=10):\n",
        "  prompts = generate_candidate_prompts(description, test_cases, number_of_prompts)\n",
        "  prompt_ratings = test_candidate_prompts(test_cases, description, prompts)\n",
        "\n",
        "  # Print the final ELO ratings\n",
        "  table = PrettyTable()\n",
        "  table.field_names = [\"Prompt\", \"Rating\"]\n",
        "  for prompt, rating in sorted(prompt_ratings.items(), key=lambda item: item[1], reverse=True):\n",
        "      table.add_row([prompt, rating])\n",
        "\n",
        "  print(table)"
      ],
      "metadata": {
        "id": "wXeqMQpzzosx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In the cell below, fill in your description, test cases, and number of prompts to generate."
      ],
      "metadata": {
        "id": "MJSSKFfV_X9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "description = \"Given a prompt, generate a landing page headline.\" # this style of description tends to work well\n",
        "\n",
        "test_cases = [\n",
        "    {\n",
        "        'prompt': 'Promoting an innovative new fitness app, Smartly',\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Why a vegan diet is beneficial for your health',\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Introducing a new online course on digital marketing',\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Launching a new line of eco-friendly clothing',\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Promoting a new travel blog focusing on budget travel',\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Advertising a new software for efficient project management',\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Introducing a new book on mastering Python programming',\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Promoting a new online platform for learning languages',\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Advertising a new service for personalized meal plans',\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Launching a new app for mental health and mindfulness',\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "number_of_prompts = 10 # this determines how many candidate prompts to generate... the higher, the more expensive, but the better the results will be\n",
        "\n",
        "generate_optimal_prompt(description, test_cases, number_of_prompts)"
      ],
      "metadata": {
        "id": "vCZvLyDepxFP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}