{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshumer/gpt-prompt-engineer/blob/main/gpt_prompt_engineer_Classification_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Ey7JZ5iLo1"
      },
      "source": [
        "# gpt-prompt-engineer -- Classification Version\n",
        "By Matt Shumer (https://twitter.com/mattshumer_)\n",
        "\n",
        "Github repo: https://github.com/mshumer/gpt-prompt-engineer\n",
        "\n",
        "Generate an optimal prompt for a given classification task that can be evaluated with 'true'/'false' outputs.\n",
        "\n",
        "You just need to describe the task clearly, and provide some test cases (for example, if we're classifying statements as 'happy' or not, a 'true' test case could be \"I had a great day!\", and a 'false' test case could be \"I am feeling gloomy.\").\n",
        "\n",
        "To generate a prompt:\n",
        "1. In the first cell, add in your OpenAI key.\n",
        "2. If you don't have GPT-4 access, change `model='gpt-4'` in the second cell to `model='gpt-3.5-turbo'`. If you do have access, skip this step.\n",
        "2. In the last cell, fill in the description of your task, as many test cases as you want (test cases are example prompts and their expected output), and the number of prompts to generate.\n",
        "3. Run all the cells! The AI will generate a number of candidate prompts, and test them all to find the best one!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openai prettytable tqdm tenacity wandb -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UW3ztLRsolnk"
      },
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable\n",
        "import time\n",
        "import wandb\n",
        "import openai\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# openai.api_key = \"ADD YOUR KEY HERE\" # enter your OpenAI API key here\n",
        "openai.api_key = \"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\"\n",
        "use_wandb = True # set to True if you want to use wandb to log your config and results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "candidate_gen_system_prompt = \"\"\"Your job is to generate system prompts for GPT-4, given a description of the use-case and some test cases.\n",
        "\n",
        "The prompts you will be generating will be for classifiers, with 'true' and 'false' being the only possible outputs.\n",
        "\n",
        "In your generated prompt, you should describe how the AI should behave in plain English. Include what it will see, and what it's allowed to output. Be creative in with prompts to get the best possible results. The AI knows it's an AI -- you don't need to tell it this.\n",
        "\n",
        "You will be graded based on the performance of your prompt... but don't cheat! You cannot include specifics about the test cases in your prompt. Any prompts with examples will be disqualified.\n",
        "\n",
        "Most importantly, output NOTHING but the prompt. Do not include anything else in your message.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "CANDIDATE_MODEL = 'gpt-4'\n",
        "CANDIDATE_MODEL_TEMPERATURE = 0.9\n",
        "\n",
        "EVAL_MODEL = 'gpt-3.5-turbo'\n",
        "EVAL_MODEL_TEMPERATURE = 0\n",
        "EVAL_MODEL_MAX_TOKENS = 1\n",
        "\n",
        "NUMBER_OF_PROMPTS = 10 # this determines how many candidate prompts to generate... the higher, the more expensive\n",
        "\n",
        "N_RETRIES = 3  # number of times to retry a call to the ranking model if it fails\n",
        "\n",
        "WANDB_PROJECT_NAME = \"gpt-prompt-eng\" # used if use_wandb is True, Weights &| Biases project name\n",
        "WANDB_RUN_NAME = None # used if use_wandb is True, optionally set the Weights & Biases run name to identify this run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def start_wandb_run():\n",
        "  # start a new wandb run and log the config\n",
        "  wandb.init(\n",
        "    project=WANDB_PROJECT_NAME, \n",
        "    name=WANDB_RUN_NAME,\n",
        "    config={\n",
        "      \"candidate_gen_system_prompt\": candidate_gen_system_prompt, \n",
        "      \"candiate_model\": CANDIDATE_MODEL,\n",
        "      \"candidate_model_temperature\": CANDIDATE_MODEL_TEMPERATURE,\n",
        "      \"generation_model\": EVAL_MODEL,\n",
        "      \"generation_model_temperature\": EVAL_MODEL_TEMPERATURE,\n",
        "      \"generation_model_max_tokens\": EVAL_MODEL_MAX_TOKENS,\n",
        "      \"n_retries\": N_RETRIES,\n",
        "      \"number_of_prompts\": NUMBER_OF_PROMPTS\n",
        "      })\n",
        "  \n",
        "  return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmorgan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/morganmcguire/ML/gpt-prompt-engineer/wandb/run-20230711_181259-7ph8ehfn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/morgan/gpt-prompt-eng/runs/7ph8ehfn' target=\"_blank\">prime-bush-8</a></strong> to <a href='https://wandb.ai/morgan/gpt-prompt-eng' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/morgan/gpt-prompt-eng' target=\"_blank\">https://wandb.ai/morgan/gpt-prompt-eng</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/morgan/gpt-prompt-eng/runs/7ph8ehfn' target=\"_blank\">https://wandb.ai/morgan/gpt-prompt-eng/runs/7ph8ehfn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Optional logging to Weights & Biases to reocrd the configs, prompts and results\n",
        "if use_wandb:\n",
        "  start_wandb_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KTRFiBhSouz8"
      },
      "outputs": [],
      "source": [
        "# Get Score - retry up to N_RETRIES times, waiting exponentially between retries.\n",
        "@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))\n",
        "def generate_candidate_prompts(description, test_cases, number_of_prompts):\n",
        "  outputs = openai.ChatCompletion.create(\n",
        "      model=CANDIDATE_MODEL,\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": candidate_gen_system_prompt},\n",
        "          {\"role\": \"user\", \"content\": f\"Here are some test cases:`{test_cases}`\\n\\nHere is the description of the use-case: `{description.strip()}`\\n\\nRespond with your prompt, and nothing else. Be creative.\"}\n",
        "          ],\n",
        "      temperature=CANDIDATE_MODEL_TEMPERATURE,\n",
        "      n=number_of_prompts)\n",
        "\n",
        "  prompts = []\n",
        "\n",
        "  for i in outputs.choices:\n",
        "    prompts.append(i.message.content)\n",
        "  return prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "w4ltgxntszwK"
      },
      "outputs": [],
      "source": [
        "def test_candidate_prompts(test_cases, prompts):\n",
        "  prompt_results = {prompt: {'correct': 0, 'total': 0} for prompt in prompts}\n",
        "\n",
        "  # Initialize the table\n",
        "  table = PrettyTable()\n",
        "  table_field_names = [\"Prompt\", \"Expected\"] + [f\"Prompt {i+1}-{j+1}\" for j, prompt in enumerate(prompts) for i in range(prompts.count(prompt))]\n",
        "  table.field_names = table_field_names\n",
        "\n",
        "  # Wrap the text in the \"Prompt\" column\n",
        "  table.max_width[\"Prompt\"] = 100\n",
        "\n",
        "  if use_wandb:\n",
        "    wandb_table = wandb.Table(columns=table_field_names)\n",
        "    if wandb.run is None:\n",
        "      start_wandb_run()\n",
        "\n",
        "  for test_case in test_cases:\n",
        "      row = [test_case['prompt'], test_case['answer']]\n",
        "      for prompt in prompts:\n",
        "          x = openai.ChatCompletion.create(\n",
        "              model=EVAL_MODEL,\n",
        "              messages=[\n",
        "                  {\"role\": \"system\", \"content\": prompt},\n",
        "                  {\"role\": \"user\", \"content\": f\"{test_case['prompt']}\"}\n",
        "              ],\n",
        "              logit_bias={\n",
        "                  '1904': 100,  # 'true' token\n",
        "                  '3934': 100,  # 'false' token\n",
        "              },\n",
        "              max_tokens=EVAL_MODEL_MAX_TOKENS,\n",
        "              temperature=EVAL_MODEL_TEMPERATURE,\n",
        "          ).choices[0].message.content\n",
        "\n",
        "\n",
        "          status = \"✅\" if x == test_case['answer'] else \"❌\"\n",
        "          row.append(status)\n",
        "\n",
        "          # Update model results\n",
        "          if x == test_case['answer']:\n",
        "              prompt_results[prompt]['correct'] += 1\n",
        "          prompt_results[prompt]['total'] += 1\n",
        "\n",
        "      table.add_row(row)\n",
        "      if use_wandb:\n",
        "        wandb_table.add_data(*row)\n",
        "\n",
        "  print(table)\n",
        "\n",
        "  # Calculate and print the percentage of correct answers and average time for each model\n",
        "  best_prompt = None\n",
        "  best_percentage = 0\n",
        "  for i, prompt in enumerate(prompts):\n",
        "      correct = prompt_results[prompt]['correct']\n",
        "      total = prompt_results[prompt]['total']\n",
        "      percentage = (correct / total) * 100\n",
        "      print(f\"Prompt {i+1} got {percentage:.2f}% correct.\")\n",
        "      if percentage > best_percentage:\n",
        "          best_percentage = percentage\n",
        "          best_prompt = prompt\n",
        "\n",
        "  if use_wandb: # log the results to a Weights & Biases table and finsih the run\n",
        "    wandb.log({\"prompt_ratings\": wandb_table})\n",
        "    wandb.config.update({\"best_prompt\": best_prompt, \"best_percentage\": best_percentage})\n",
        "    wandb.finish()\n",
        "\n",
        "  print(f\"The best prompt was '{best_prompt}' with a correctness of {best_percentage:.2f}%.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SBJEi1hkrT9T"
      },
      "outputs": [],
      "source": [
        "test_cases = [\n",
        "    {\n",
        "        'prompt': 'Find the best contact email on this site.',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'who is the current president?',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'order me a pizza',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'what are some ways a doctor could use an assistant?',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'write a speech on the danger of cults',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Make a reservation at The Accent for 9pm',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'organize my google drive',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Find the highest-rated Italian restaurant near me.',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Explain the theory of relativity.',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'What are the main differences between Python and Java programming languages?',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Translate the following English sentence to Spanish: \"The weather today is great.\"',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Create a new event on my calendar for tomorrow at 2 pm.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Write a short story about a lonely cowboy.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Design a logo for a startup.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Compose a catchy jingle for a new soda brand.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    # {\n",
        "    #     'prompt': 'Calculate the square root of 1999.',\n",
        "    #     'answer': 'false'\n",
        "    # },\n",
        "    # {\n",
        "    #     'prompt': 'What are the health benefits of yoga?',\n",
        "    #     'answer': 'true'\n",
        "    # },\n",
        "    # {\n",
        "    #     'prompt': 'find me a source of meat that can be shipped to canada',\n",
        "    #     'answer': 'true'\n",
        "    # },\n",
        "    # {\n",
        "    #     'prompt': 'Find the best-selling book of all time.',\n",
        "    #     'answer': 'true'\n",
        "    # },\n",
        "    # {\n",
        "    #     'prompt': 'What are the top 5 tourist attractions in Brazil?',\n",
        "    #     'answer': 'true'\n",
        "    # },\n",
        "    # {\n",
        "    #     'prompt': 'List the main ingredients in a traditional lasagna recipe.',\n",
        "    #     'answer': 'true'\n",
        "    # },\n",
        "    # {\n",
        "    #     'prompt': 'How does photosynthesis work in plants?',\n",
        "    #     'answer': 'true'\n",
        "    # },\n",
        "    # {\n",
        "    #     'prompt': 'Write a Python program to reverse a string.',\n",
        "    #     'answer': 'false'\n",
        "    # },\n",
        "    # {\n",
        "    #     'prompt': 'Create a workout routine for a beginner.',\n",
        "    #     'answer': 'false'\n",
        "    # },\n",
        "    # {\n",
        "    #     'prompt': 'Edit my resume to highlight my project management skills.',\n",
        "    #     'answer': 'false'\n",
        "    # },\n",
        "    # {\n",
        "    #     'prompt': 'Draft an email to a client to discuss a new proposal.',\n",
        "    #     'answer': 'false'\n",
        "    # },\n",
        "    # {\n",
        "    #     'prompt': 'Plan a surprise birthday party for my best friend.',\n",
        "    #     'answer': 'false'\n",
        "    # }]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "ename": "ServiceUnavailableError",
          "evalue": "The server is overloaded or not ready yet.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mServiceUnavailableError\u001b[0m                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m     wandb\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m\"\u001b[39m: description, \n\u001b[1;32m      5\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtest_cases\u001b[39m\u001b[39m\"\u001b[39m: test_cases})\n\u001b[1;32m      7\u001b[0m candidate_prompts \u001b[39m=\u001b[39m generate_candidate_prompts(description, test_cases, NUMBER_OF_PROMPTS)\n\u001b[0;32m----> 8\u001b[0m test_candidate_prompts(test_cases, candidate_prompts)\n",
            "Cell \u001b[0;32mIn[25], line 20\u001b[0m, in \u001b[0;36mtest_candidate_prompts\u001b[0;34m(test_cases, prompts)\u001b[0m\n\u001b[1;32m     18\u001b[0m row \u001b[39m=\u001b[39m [test_case[\u001b[39m'\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m'\u001b[39m], test_case[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m---> 20\u001b[0m     x \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     21\u001b[0m         model\u001b[39m=\u001b[39;49mEVAL_MODEL,\n\u001b[1;32m     22\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     23\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt},\n\u001b[1;32m     24\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mtest_case[\u001b[39m'\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m}\n\u001b[1;32m     25\u001b[0m         ],\n\u001b[1;32m     26\u001b[0m         logit_bias\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     27\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39m1904\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m100\u001b[39;49m,  \u001b[39m# 'true' token\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39m3934\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m100\u001b[39;49m,  \u001b[39m# 'false' token\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m         },\n\u001b[1;32m     30\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mEVAL_MODEL_MAX_TOKENS,\n\u001b[1;32m     31\u001b[0m         temperature\u001b[39m=\u001b[39;49mEVAL_MODEL_TEMPERATURE,\n\u001b[1;32m     32\u001b[0m     )\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n\u001b[1;32m     35\u001b[0m     status \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m✅\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m x \u001b[39m==\u001b[39m test_case[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m❌\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     row\u001b[39m.\u001b[39mappend(status)\n",
            "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.11/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.11/site-packages/openai/api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    612\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    613\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    616\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 619\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    620\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    623\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    624\u001b[0m         ),\n\u001b[1;32m    625\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    626\u001b[0m     )\n",
            "File \u001b[0;32m~/mambaforge/envs/ml/lib/python3.11/site-packages/openai/api_requestor.py:662\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m OpenAIResponse(\u001b[39mNone\u001b[39;00m, rheaders)\n\u001b[1;32m    661\u001b[0m \u001b[39mif\u001b[39;00m rcode \u001b[39m==\u001b[39m \u001b[39m503\u001b[39m:\n\u001b[0;32m--> 662\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mServiceUnavailableError(\n\u001b[1;32m    663\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe server is overloaded or not ready yet.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    664\u001b[0m         rbody,\n\u001b[1;32m    665\u001b[0m         rcode,\n\u001b[1;32m    666\u001b[0m         headers\u001b[39m=\u001b[39mrheaders,\n\u001b[1;32m    667\u001b[0m     )\n\u001b[1;32m    668\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    669\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtext/plain\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m rheaders\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m'\u001b[39m):\n",
            "\u001b[0;31mServiceUnavailableError\u001b[0m: The server is overloaded or not ready yet."
          ]
        }
      ],
      "source": [
        "description = \"Decide if a task is research-heavy.\" # describe the classification task clearly\n",
        "\n",
        "# If Weights & Biases is enabled, log the description and test cases too\n",
        "if use_wandb:\n",
        "    wandb.config.update({\"description\": description, \n",
        "                        \"test_cases\": test_cases})\n",
        "\n",
        "candidate_prompts = generate_candidate_prompts(description, test_cases, NUMBER_OF_PROMPTS)\n",
        "test_candidate_prompts(test_cases, candidate_prompts)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMvbQztC95mJY9x+Gc/uEm+",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
